{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 4\n",
    "Sepehr Abbaspour\n",
    "610398147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we import 'NLTK' library for tokenisation and achieve stop words list.\n",
    "The two functions needed are called 'wordpunct_tokenize' and 'stopwords', respectively.\n",
    "Libraries 'string', sklearn, heap1, rank_bm25, math and matplotlib are imported for better preprocessing, importing vector space structure, max-heap sorting, applying Okapi BM25 algorithm, rounding recalls to tens, and generating random parameters for smoothing in language model, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from heapq import nlargest\n",
    "from rank_bm25 import BM25Okapi\n",
    "from math import prod, ceil, floor\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining preprocessor function which preprocess the terms in the documents and queries.\n",
    "In this function the input string is changed in the way of lowering its letters, elimination of the stop words and asymmetric expansion (changing from plural form to singular form in the specific rules).\n",
    "Tokenisation level is to delete symbolic letters by iterating the full-lowered string.\n",
    "Some variables are defined to be pointed for the next times.\n",
    "After that, the terms are devided and assembled to make String be the list of them and each of them gets ready for a possible changing.\n",
    "The changing contains stop word deletion in which stop words become removed from the list and replaces with null phrase (\"\").\n",
    "This replacements exist to avoid changing the length of the list 'String' while iterating it.\n",
    "After stop words deletion, we check any plural form for each non-stop word term and if its in any plural form, it'll be changed to its singular form according to official plural nouns table rules.\n",
    "At last the null phrases in the list of edited terms become removed and the final list is returned by the function.\n",
    "A bit try to preprocess on verbs in different time forms added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessor(String):\n",
    "    null, Consonants, Irregulars, stopWords = \"\", \"qwrtypsdfghjklzxcvbnm\", {\"begun\": \"begin\", \"frozen\": \"freeze\", \"children\": \"child\", \"feet\": \"foot\", \"teeth\": \"tooth\",\"mice\": \"mouse\", \"people\": \"person\"}, stopwords.words('english')\n",
    "    String = wordpunct_tokenize(String) #Case Folding & Tokenisations\n",
    "    String = [Token.lower() for Token in String if Token not in punctuation and Token.lower() not in stopWords]\n",
    "    lengthString = len(String)\n",
    "    \n",
    "    for Index in range(lengthString):\n",
    "        #Asymmetric Expansion\n",
    "        if String[Index].endswith(\"ies\") and String[Index][-4] in Consonants:\n",
    "            String[Index] = String[Index][:-3] + \"y\"\n",
    "\n",
    "        elif String[Index].endswith(\"ves\"):\n",
    "            String[Index] = String[Index][:-3] + \"f\"\n",
    "            \n",
    "        elif String[Index].endswith(\"es\") and (String[Index][-3] in [\"s\", \"x\", \"z\"] or String[Index][-4:-2] in [\"ch\", \"sh\"] or (String[Index][-3] == \"o\" and String[Index][-4] in Consonants)):\n",
    "            String[Index] = String[Index][:-2]\n",
    "        \n",
    "        elif String[Index].endswith(\"s\"):\n",
    "            String[Index] = String[Index][:-1]\n",
    "            \n",
    "        elif String[Index].endswith(\"men\"):\n",
    "            String[Index] = String[Index][:-3] + \"man\"\n",
    "\n",
    "        if String[Index].endswith(\"ed\"):\n",
    "            String[Index] = String[Index][:-2]\n",
    "            \n",
    "        elif String[Index] in Irregulars.keys():\n",
    "            String[Index] = Irregulars[String[Index]]\n",
    "\n",
    "        elif String[Index].endswith(\"en\"):\n",
    "            String[Index] = String[Index][:-2]\n",
    " \n",
    "    while null in String:\n",
    "        String.remove(null)\n",
    "        \n",
    "    return String\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're gonna contruct model functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function uses space vector model to rank the documents according to the input query and outputs the k first of best relevant documents by means of their given ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spaceVector(Query, Bound):\n",
    "    tfidfVectorizer, q, vectorScore, retrievedDocs = TfidfVectorizer(), [\" \".join(Query)], {}, []\n",
    "    d = tfidfVectorizer.fit_transform(Documents)\n",
    "    q = tfidfVectorizer.transform(q)\n",
    "    Scores = cosine_similarity(q, d)\n",
    "    for Index in range(1400):\n",
    "        vectorScore[Index] = Scores[0][Index]\n",
    "        \n",
    "    Ids, Scores = list(vectorScore.keys()), list(vectorScore.values())\n",
    "    electedScores = nlargest(Bound, Scores) #Max-heap implemention!\n",
    "    return [{\"Id\": Ids[Scores.index(Score)] + 1, \"Score\": Score} for Score in electedScores] #Retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function uses probabilistic model to rank the documents according to the input query and outputs the k first of best relevant documents by means of their given ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Probabilistic(Query, Bound):\n",
    "    BMProb = BM25Okapi(InvertedIndex)\n",
    "    Scores, electedDocuments = BMProb.get_scores(Query), BMProb.get_top_n(Query, Documents , Bound) #Also handles large queries by built-in funtions and methods.\n",
    "    return [{\"Id\": Documents.index(Document) + 1, \"Score\": Scores[Documents.index(Document)]} for Document in electedDocuments] #Retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third function uses probabilistic model to rank the documents according to the input query and outputs the k first of best relevant documents by means of their given ranks.\n",
    "It generate some random parameters for Jelinek-Mercer smoothing called 'Lambda'.\n",
    "It has an unkown error while it can't rank more than 470 first documents.\n",
    "We're suggested to use higher Lambda for short queries (considered less than 100 contained words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unigram(Query, Bound):\n",
    "    if len(Query) < 100:\n",
    "        Lambda = uniform(0.1, 1)\n",
    "    else:\n",
    "        Lambda = uniform(0, 0.1)\n",
    "                \n",
    "    Scores = [prod([Lambda * InvertedIndex[Index].count(Term) / len(InvertedIndex[Index]) + (1 - Lambda) * sum([Document.count(Term) for Document in InvertedIndex]) / TMc for Term in Query]) for Index in range(470)] #Unknown zero division error! 470 is the maximum number of documents in order to be considered in unigram language model.\n",
    "    electedScores = reversed(sorted(Scores)[-Bound:])\n",
    "    return [{\"Id\": Scores.index(Score) + 1, \"Score\": Score} for Score in electedScores] #Retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function evaluate 11-point interpolated average precision with a given list of ranked documents designated by one three models.\n",
    "The recalls ar eall rounded to the nearest tenth in range 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(Index, retrievedDocs):\n",
    "    Coordinates, AP = {}, 0\n",
    "    RDocs = set(Relevants[Index]) #Relevant items for the query\n",
    "    numberRDocs = len(RDocs)\n",
    "    for Measure in range(Bound): #Iterate on retrieved documents index by index ascendingly and then, calculate precision and recall and make coordinate '(Precision, Recall)' by the whole considered documents in each iterate.\n",
    "        Measures = retrievedDocs[:Measure + 1] \n",
    "        tp = len(set(Measures).intersection(RDocs))\n",
    "        Recall = tp / numberRDocs\n",
    "        if Recall % 1000 // 100 >= 5:\n",
    "            Recall = ceil(Recall * 10) / 10\n",
    "        else:\n",
    "            Recall = floor(Recall * 10) / 10\n",
    "            \n",
    "        if Recall not in Coordinates:\n",
    "            Coordinates[Recall] = [tp / len(Measures)]\n",
    "            continue\n",
    "        \n",
    "        Coordinates[Recall].append(tp / len(Measures))\n",
    "\n",
    "    Recalls = Coordinates.keys()\n",
    "    for Recall in Recalls:\n",
    "        AP += max(Coordinates[Recall])\n",
    "        \n",
    "    return AP / 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are defined global variables used in the functions; and also a huge list contains documents and another huge list contains whole queries.\n",
    "There're also list of really relevant documents twoard (index + 1)th query from the list of queries.\n",
    "These data structures must relate to opened and read test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Globals!\n",
    "docFile, qFile, qrFile, Relevants, nonRelevants, SVAPs, Okapi25APs, UniAPs, TMc = open(\"cran.all.1400\").read(), open(\"cran.qry\").read(), open(\"cranqrel.txt\"), {}, {}, [], [], [], 0\n",
    "InvertedIndex, Queries = docFile.split(\".I \")[1:], qFile.split(\".I \")[1:] #First element will be '[]'!\n",
    "#Preprocessing ONLY TITLE and TEXT!\n",
    "InvertedIndex, Queries = [preProcessor(Document[Document.find(\".T\") + 2 : Document.find(\".A\")] + \" \" + Document[Document.find(\".W\") + 2:]) for Document in InvertedIndex], [preProcessor(Query[7:]) for Query in Queries]\n",
    "#Total number of tokens in the collection.\n",
    "for Document in InvertedIndex:\n",
    "    TMc += len(Document)\n",
    "    \n",
    "Documents = [\" \".join(Document) for Document in InvertedIndex]\n",
    "for Line in qrFile:\n",
    "    Line = list(map(int, Line.split()))\n",
    "    if Line[2] >= 1:\n",
    "        queryTopic = Line[0]\n",
    "        if queryTopic in Relevants:\n",
    "            Relevants[queryTopic].append(Line[1])\n",
    "            continue\n",
    "        \n",
    "        Relevants[queryTopic] = [Line[1]]\n",
    "        continue\n",
    "    \n",
    "    nonRelevants[queryTopic] = Line[1]\n",
    "\n",
    "qrFile.close()\n",
    "while True:\n",
    "    try:\n",
    "        Bound = int(input(\"How many documents do you need them be retrieved? \"))\n",
    "        if Bound >= 0:\n",
    "            break\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Enter a non-negative integer!\")\n",
    "\n",
    "print(f\"{InvertedIndex}\\n\")\n",
    "print(f\"{Documents}\\n\")\n",
    "print(f\"{Relevants}\\n\")\n",
    "print(f\"{nonRelevants}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating the list of queries and evaluate whatever needed and then evaluate MAP for each model.\n",
    "The bigger MAP does a model have, the better model for a great Information Retrieval system it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Query in Queries:\n",
    "    SV, Okapi25, Uni, realqIndex = spaceVector(Query, Bound), Probabilistic(Query, Bound), Unigram(Query, Bound), Queries.index(Query) + 1\n",
    "    print(f\"Space Vector Model retrieved {Bound} documents may be relevant with {realqIndex}th query: {SV}\\n\")\n",
    "    print(f\"Probabilistic Model 'Okapi BM25' retrieved {Bound} documents may be relevant with {realqIndex}th query: {Okapi25}\\n\")\n",
    "    print(f\"Smoothed Unigram Language Model {Bound} documents may be relevant with {realqIndex}th query: {Uni}\\n\")\n",
    "    \n",
    "    SVAPs.append(Evaluation(realqIndex, [Retrieved[\"Id\"] for Retrieved in SV]))\n",
    "    Okapi25APs.append(Evaluation(realqIndex, [Retrieved[\"Id\"] for Retrieved in Okapi25]))\n",
    "    UniAPs.append(Evaluation(realqIndex, [Retrieved[\"Id\"] for Retrieved in Uni]))\n",
    "\n",
    "print(f\"11-point interpolated average precision for the whole queries {SVAPs}\\n\")\n",
    "print(f\"MAP = {sum(SVAPs) / len(SVAPs)}\\n\")\n",
    "print(f\"11-point interpolated average precision for the whole queries {Okapi25APs}\\n\")\n",
    "print(f\"MAP = {sum(Okapi25APs) / len(Okapi25APs)}\\n\")\n",
    "print(f\"11-point interpolated average precision for the whole queries {UniAPs}\\n\")\n",
    "print(f\"MAP = {sum(UniAPs) / len(UniAPs)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is in an optimal form due to the pointed conditions in the constructions of the defined functions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
